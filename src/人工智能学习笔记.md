# 人工智能学习笔记

## 概念梳理

### 预训练模型

在一个原始任务上预先训练一个初始模型，然后在目标任务上使用该模型。这本质上是一种迁移学习的方法。

在自己的目标任务上使用别人训练好的模型，针对目标任务的特性，对该初始模型进行精调，从而达到提高目标任务的目的。

#### 自编码语言模型（Autoencoder Language Model）

根据上文内容预测下一个可能的单词，就是常说的自左向右的语言模型任务，或者反过来也行，就是根据下文预测前面的单词。GPT 就是典型的自回归语言模型。

**优点**

其实跟下游NLP任务有关，比如生成类NLP任务，比如文本摘要，机器翻译等，在实际生成内容的时候，就是从左向右的，自回归语言模型天然匹配这个过程。

**缺点**

只能利用上文或者下文的信息，不能同时利用上文和下文的信息。

##### BERT

即双向Transformer的Encoder，bert并没有使用到transformer的decoder。模型的主要创新点集中在pre-train方法上，可以使用大量的无监督数据，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。

###### transformer

###### Masked LM（Masked Language Model）

此任务就是**随机遮盖或替换**一句话里面的任意字或词，然后让模型通过上下文预测那一个被遮盖或替换的部分，之后**做 Loss 的时候也只计算被遮盖部分的 Loss**，实际操作如下：

1. 随机把一句话中 15% 的 token（字或词）替换成以下内容：

2. 1. 这些 token 有 80% 的几率被替换成 `[MASK]`，例如 my dog is hairy→my dog is [MASK]
   2. 有 10% 的几率被替换成任意一个其它的 token，例如 my dog is hairy→my dog is apple
   3. 有 10% 的几率原封不动，例如 my dog is hairy→my dog is hairy

3. 之后让模型**预测和还原**被遮盖掉或替换掉的部分，计算损失的时候，只计算在第 1 步里被**随机遮盖或替换**的部分，其余部分不做损失。

这样做的好处是，BERT 并不知道 [MASK] 替换的是哪一个词，而且任何一个词都有可能是被替换掉的，比如它看到的 apple 可能是被替换的词。这样强迫模型在编码当前时刻词的时候不能太依赖当前的词，而要考虑它的上下文，甚至根据上下文进行 "纠错"。比如上面的例子中，模型在编码 apple 时，根据上下文 my dog is，应该把 apple 编码成 hairy 的语义而不是 apple 的语义。

###### Next Sentence Prediction

判断两个句子（A和B）是否为前后句关系，之后要在这两个句子中加一些特殊的 token：[CLS]A[SEP]B[SEP]。也就是在句子开头加一个 [CLS]，在两句话之间和句末加 [SEP]。拼接起来后，如果句子超出长度，需要从头部或者尾部截取，不能从中间截取，保证句子的完整性。最终归结为二分类问题，需要让正负例的两种情况出现的数量为 **1:1**。

- 正例：属于上下文的一对句子A和B。
- 负例：第一个句子选择A，第二个句子从其他段落中选择一个句子。

#### 自回归语言模型（Autoregressive Language Model）

自编码语言模型是对输入的句子随机Mask其中的单词，然后预训练过程的主要任务之一是根据上下文单词来预测这些被Mask掉的单词，那些被Mask掉的单词就是在输入侧加入的噪音。BERT就是典型的自编码类语言模型。

**优点**

它能比较自然地融入双向语言模型，同时看到被预测单词的上文和下文。

**缺点**

主要在输入侧引入[Mask]标记，导致预训练阶段和Fine-tuning阶段不一致的问题，因为Fine-tuning阶段是看不到[Mask]标记的。而Bert这种自编码模式，在生成类NLP任务中，就面临训练过程和应用过程不一致的问题，导致生成类的NLP任务到目前为止都做不太好。